///*
// * Licensed to the Apache Software Foundation (ASF) under one or more
// * contributor license agreements.  See the NOTICE file distributed with
// * this work for additional information regarding copyright ownership.
// * The ASF licenses this file to You under the Apache License, Version 2.0
// * (the "License"); you may not use this file except in compliance with
// * the License.  You may obtain a copy of the License at
// *
// *    http://www.apache.org/licenses/LICENSE-2.0
// *
// * Unless required by applicable law or agreed to in writing, software
// * distributed under the License is distributed on an "AS IS" BASIS,
// * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// * See the License for the specific language governing permissions and
// * limitations under the License.
// */
//
//package com.sposter.mllib
//
//import org.apache.spark.mllib.recommendation.{ALS, MatrixFactorizationModel, Rating}
//import org.apache.spark.rdd.RDD
//import org.apache.spark.{SparkConf, SparkContext}
//import scopt.OptionParser
//
//import scala.collection.mutable
//
///**
// * An example app for ALS on MovieLens data (http://grouplens.org/datasets/movielens/).
// * Run with
// * {{{
// * bin/run-example org.apache.spark.examples.mllib.MovieLensALS
// * }}}
// * A synthetic dataset in MovieLens format can be found at `data/mllib/sample_movielens_data.txt`.
// * If you use it as a template to create your own app, please use `spark-submit` to submit your app.
// */
//object MovieLensALS {
//
//  case class Params(
//                     input: String = "data/ml-1m/ratings.dat",
//                     userDataInput: String = "data/ml-1m/personalRatings.txt",
//                     kryo: Boolean = false,
//                     numIterations: Int = 20,
//                     lambda: Double = 1.0,
//                     rank: Int = 10,
//                     numUserBlocks: Int = -1,
//                     numProductBlocks: Int = -1,
//                     implicitPrefs: Boolean = false)
//
//  def main(args: Array[String]) {
//    val defaultParams = Params()
//
//    val parser = new OptionParser[Params]("MovieLensALS") {
//      head("MovieLensALS: an example app for ALS on MovieLens data.")
//      opt[Int]("rank")
//        .text(s"rank, default: ${defaultParams.rank}}")
//        .action((x, c) => c.copy(rank = x))
//      opt[Int]("numIterations")
//        .text(s"number of iterations, default: ${defaultParams.numIterations}")
//        .action((x, c) => c.copy(numIterations = x))
//      opt[Double]("lambda")
//        .text(s"lambda (smoothing constant), default: ${defaultParams.lambda}")
//        .action((x, c) => c.copy(lambda = x))
//      opt[Unit]("kryo")
//        .text("use Kryo serialization")
//        .action((_, c) => c.copy(kryo = true))
//      opt[Int]("numUserBlocks")
//        .text(s"number of user blocks, default: ${defaultParams.numUserBlocks} (auto)")
//        .action((x, c) => c.copy(numUserBlocks = x))
//      opt[Int]("numProductBlocks")
//        .text(s"number of product blocks, default: ${defaultParams.numProductBlocks} (auto)")
//        .action((x, c) => c.copy(numProductBlocks = x))
//      opt[Unit]("implicitPrefs")
//        .text("use implicit preference")
//        .action((_, c) => c.copy(implicitPrefs = true))
//      opt[String]("userDataInput")
//        .required()
//        .text("input paths to user dataset")
//        .action((x, c) => c.copy(userDataInput = x))
//      arg[String]("<input>")
//        .required()
//        .text("input paths to a MovieLens dataset of ratings")
//        .action((x, c) => c.copy(input = x))
//      note(
//        """
//          |For example, the following command runs this app on a synthetic dataset:
//          |
//          | bin/spark-submit --class com.javachen.grab.examples.mllib.MovieLensALS \
//          |  examples/target/scala-*/grab-examples-*.jar \
//          |  --rank 5 --numIterations 20 --lambda 1.0 \
//          |  --userDataInput data/ml-1m/personalRatings.txt \
//          |  data/ml-1m/ratings.dat
//        """.stripMargin)
//    }
//
//    parser.parse(args, defaultParams).map { params =>
//      run(params)
//    } getOrElse {
//      System.exit(1)
//    }
//  }
//
//  def run(params: Params) {
//    val conf = new SparkConf().setAppName(s"MovieLensALS with $params").set("spark.executor.memory", "2g")
//    if (params.kryo) {
//      conf.registerKryoClasses(Array(classOf[mutable.BitSet], classOf[Rating]))
//        .set("spark.kryoserializer.buffer.mb", "8")
//    }
//    val sc = new SparkContext(conf)
//
//
//    val ratings = sc.textFile(params.input).map { line =>
//      val fields = line.split("::")
//      /*
//        * MovieLens ratings are on a scale of 1-5:
//        * 5: Must see
//        * 4: Will enjoy
//        * 3: It's okay
//        * 2: Fairly bad
//        * 1: Awful
//        * So we should not recommend a movie if the predicted rating is less than 3.
//        * To map ratings to confidence scores, we use
//        * 5 -> 2.5, 4 -> 1.5, 3 -> 0.5, 2 -> -0.5, 1 -> -1.5. This mappings means unobserved
//        * entries are generally between It's okay and Fairly bad.
//        * The semantics of 0 in this expanded world of non-positive weights
//        * are "the same as never having interacted at all".
//        */
//      if (params.implicitPrefs) {
//        // format: (timestamp % 10, Rating(userId, movieId, rating))
//        (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble - 2.5))
//      } else {
//        // format: (timestamp % 10, Rating(userId, movieId, rating))
//        (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))
//      }
//    }.cache()
//
//
//    val numRatings = ratings.count()
//    val numUsers = ratings.map(_._2.user).distinct().count()
//    val numMovies = ratings.map(_._2.product).distinct().count()
//
//    println("Got " + numRatings + " ratings from " + numUsers + " users on " + numMovies + " movies.")
//
//    val model = evaluateMode(params, ratings)
//
//    predictMoive(params, sc, model)
//
//    // clean up
//    sc.stop()
//  }
//
//  def predictMoive(params: Params, sc: SparkContext, model: MatrixFactorizationModel): Unit = {
//    //为用户1推荐10个
//    var rs = model.recommendProducts(1, 10)
//    var value = ""
//    var key = 0
//
//    //保存推荐数据到hbase中
//    rs.foreach(r => {
//      key = r.user
//      value = value + r.product + ":" + r.rating + ","
//    })
//
//    println(value)
//
//  }
//
//  def evaluateMode(params: Params, ratings: RDD[(Long, Rating)]): MatrixFactorizationModel = {
//    val training = ratings.values.repartition(4)
//
//    //建立模型
//    val start = System.currentTimeMillis()
//    val model = new ALS().setRank(params.rank).setIterations(params.numIterations).setLambda(params.lambda).setImplicitPrefs(params.implicitPrefs).setUserBlocks(params.numUserBlocks).setProductBlocks(params.numProductBlocks).run(training)
//    println("Train Time = " + (System.currentTimeMillis() - start) * 1.0 / 1000)
//    val testRmse = computeRmse(model, training)
//
//    println("RMSE = " + testRmse)
//
//    model
//  }
//
//  /** Compute RMSE (Root Mean Squared Error). */
//   def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating]) = {
//    val usersProducts = data.map { case Rating(user, product, rate) =>
//      (user, product)
//    }
//
//    val predictions = model.predict(usersProducts).map { case Rating(user, product, rate) =>
//      ((user, product), rate)
//    }
//
//    val ratesAndPreds = data.map { case Rating(user, product, rate) =>
//      ((user, product), rate)
//    }.join(predictions).sortByKey()
//
//    math.sqrt(ratesAndPreds.map { case ((user, product), (r1, r2)) =>
//      val err = (r1 - r2)
//      err * err
//    }.mean())
//  }
//}
